{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqbNoBdRgEKy8Pju/kFTj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day19_MLP_in_PyTorch_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day19\n",
        "## Building a Simple MLP using PyTorch Library\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n"
      ],
      "metadata": {
        "id": "8TmDLmOfqmAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "We can use PyTorch Framework to build and train MLPs and other neural networks such as CNN, RNN, Transformers. Let's learn the basics of PyTorch."
      ],
      "metadata": {
        "id": "9r7g0RAbrLTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "v1CBrgItrNzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime-->Change runtime type-->GPU or TPU_"
      ],
      "metadata": {
        "id": "3awNnb-YtDpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if torch.cuda is available, otherwise it will use CPU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "# if it prints 'cuda' then colab is running using GPU device"
      ],
      "metadata": {
        "id": "7KRrGdHZtKK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Group activity__\n",
        "Make another simple MLP with the specifications below and perform the 'Forward Pass' of the MLP."
      ],
      "metadata": {
        "id": "Ydjmn67Q3h5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate 1 random samples of (x1, x2, x3) for the network\n",
        "torch.manual_seed(0)                      # for reproducibility\n",
        "random_X = torch.randn(1,3)"
      ],
      "metadata": {
        "id": "3D6Am3aSkTqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0) # for reproducibility\n",
        "# Q1: how many hidden layers should be there? (depth)\n",
        "# answer: there is only 1 hidden layer\n",
        "num_of_hidden_layer = 1\n",
        "\n",
        "\n",
        "# Q2: how many neurons should be in each layer? (width)\n",
        "# answer: there are 3 neurons in the input  layer\n",
        "#         there are 4 neurons in the hidden layer\n",
        "#         there are 1 neurons in the output layer\n",
        "num_of_neurons_input_layer  =\n",
        "num_of_neurons_hidden_layer =\n",
        "num_of_neurons_output_layer =\n",
        "\n",
        "\n",
        "# Q3 how many dense connections should be there in between each adjacent layers\n",
        "# answer: there should be ?x? dense connnections (between input  layer and hidden layer: dense_connections_W1)\n",
        "#         there should be ?x1 dense connnections (between hidden layer and output layer: dense_connections_W2)\n",
        "# add the bias terms for all the layers except input layer\n",
        "\n",
        "\n",
        "# Q4: what should the activation be at each of the intermediate layers?\n",
        "# answer: let use sigmoid() activation function in the hidden layer\n",
        "\n",
        "# Q5: what should be activation of the final layer (let's assume we are using a binary classification task for which sigmoid activation is used)\n"
      ],
      "metadata": {
        "id": "GhtxxDoZ3ev6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do the Forward Pass in Multilayer Perceptron (MLP)\n",
        "# Step 1 -- input to hidden layer\n",
        "\n",
        "\n",
        "# Step 2 -- hidden to output layer"
      ],
      "metadata": {
        "id": "UIY0ndvQkGx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PjU46rJrpQZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Linear Layers using PyTorch"
      ],
      "metadata": {
        "id": "fPGYgJT1pfRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's build a linear layer**"
      ],
      "metadata": {
        "id": "dNxXEEFkpi7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2)                    # for reproducibility\n",
        "# construction of a linear layer\n",
        "input_layer_1 = nn.Linear(2, 4)"
      ],
      "metadata": {
        "id": "LVRZfQeVptyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Inspecting the weights of a linear layer**"
      ],
      "metadata": {
        "id": "vMRLvywQvn6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the weights of the linear layer\n",
        "print(f'Weights: \\n{input_layer_1.weight.data}')\n",
        "\n",
        "# Print the biases of the linear layer (if they exist)\n",
        "print(f'Biases: \\n{input_layer_1.bias.data}')"
      ],
      "metadata": {
        "id": "KVDaVYuGvrhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's generate a random input for our linear layer and plug it into our layer**"
      ],
      "metadata": {
        "id": "Mvhdn_kSvyvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: let's generate 1 random samples of (x1, x2) for the above linear network\n",
        "torch.manual_seed(2) # for reproducibility (you will get the same random number every time you run this cell)\n",
        "number_of_samples     = 1\n",
        "random_input          = torch.randn(number_of_samples, 2)\n",
        "print(f'input numbers: \\n{random_input.numpy()}\\n')\n",
        "\n",
        "# Step 2: apply forward pass through the network\n",
        "output = input_layer_1(random_input)\n",
        "print(f'output layer value: \\n{output.data.numpy()}\\n')\n"
      ],
      "metadata": {
        "id": "9r5YemSXv41r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Challenge #1**\n",
        "Create a new Linear layer with the following structure:\n",
        "\n",
        "> The first layer has 2 input nodes and 16 output nodes.\n"
      ],
      "metadata": {
        "id": "rhF9xI8Dx06c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2)                    # for reproducibility\n",
        "# construction of a linear layer\n",
        "# your code here"
      ],
      "metadata": {
        "id": "d_pQTklW6Esu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Challenge #2**\n",
        "\n",
        "> generate a random input for our linear layer and apply forward pass through the network. Print out the resulting output layer values. How many numbers do you expect?"
      ],
      "metadata": {
        "id": "4i94s0CiyRJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0) # for reproducibility\n",
        "# your code here"
      ],
      "metadata": {
        "id": "gqeVmz5_6Lny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Part 2 of Lecture"
      ],
      "metadata": {
        "id": "ykFebK-uQvMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's add an activation function such as *ReLu(), tanh(), or sigmoid()* after your linear layer and run the experiment again to see how it changes the outputs.**"
      ],
      "metadata": {
        "id": "NrlG1w0LjlGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construction of a linear layer\n",
        "torch.manual_seed(2) # for reproducibility (you will get the same random number every time you run this cell)\n",
        "\n",
        "input_linear_layer = nn.Linear(2, 4)  # linear transformation module (input=2, output=4)\n",
        "\n",
        "# Step 1: let's generate some random samples of (x1, x2) for the above linear network\n",
        "number_of_samples = 1\n",
        "random_X = torch.randn(number_of_samples, 2)\n",
        "print('input numbers:')\n",
        "print(random_X.numpy())\n",
        "\n",
        "sigmoid_activation = nn.Sigmoid() #this is like a call to a constructor\n",
        "tanh_activation = nn.Tanh()\n",
        "relu_activation  = nn.ReLU()"
      ],
      "metadata": {
        "id": "LhpZlUGPjqSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sigmoid()**"
      ],
      "metadata": {
        "id": "2F0pp1-9opXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: apply forward pass through the network\n",
        "output = input_linear_layer(random_X)\n",
        "print('output layer value: ')\n",
        "print(output.data.numpy())\n",
        "\n",
        "output_after_activation = sigmoid_activation(output) ## apply the activation function!\n",
        "print('Sigmoid activation value: ')\n",
        "print(output_after_activation.data.numpy())"
      ],
      "metadata": {
        "id": "fUn_yTxfqJrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise #4a : apply the Tanh() activation functions to the above network**\n",
        "\n",
        "# **Exercise #4b : apply the ReLU() activation function to the above network**\n",
        "\n",
        "Which activation function gives the result of all non-negative values?\n"
      ],
      "metadata": {
        "id": "RRnWGTs6pDCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's build the simple 1-hidden layer feedforward neural network!**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_examle_wo_weights.png\" width=400/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8jNPsQTlsyjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make the simple 1-Hidden layer feedforwrd neural network from the lecture slides\n",
        "# only with ReLU activation for hidden layer, and no (linear) activation for output layer\n",
        "\n",
        "torch.manual_seed(2) # for reproducibility (you will get the same random number every time you run this cell)\n",
        "\n",
        "# construction\n",
        "input_linear_layer  = nn.Linear(2, 3) # linear tranformation (input 2, output=1)\n",
        "relu_activation  = nn.ReLU()\n",
        "output_linear_layer = nn.Linear(3, 1) # linear transformation module (input=3, output=1)\n"
      ],
      "metadata": {
        "id": "7ifJXL9nuzd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We can apply a tensor through the Linear layers now**"
      ],
      "metadata": {
        "id": "qXjuRBT6vdR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate 2 random samples of (x1, x2) for the above linear network\n",
        "torch.manual_seed(2)\n",
        "number_of_samples = 1\n",
        "random_X = torch.randn(number_of_samples, 2) # you could imagine that these are pairs of (x1, x2) as shown in the above table\n",
        "print('input numbers:')\n",
        "print(random_X.numpy())"
      ],
      "metadata": {
        "id": "xvrVZVoavhvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply forward pass through the network\n",
        "hidden_layer_output = input_linear_layer(random_X)\n",
        "hidden_layer_output_relu = relu_activation(hidden_layer_output)\n",
        "output = output_linear_layer(hidden_layer_output_relu)\n",
        "print('Output layer result: ', output.data)"
      ],
      "metadata": {
        "id": "b6nw27mN81f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing ... nn.Sequential"
      ],
      "metadata": {
        "id": "2azscuX8wHjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creation of our network\n",
        "torch.manual_seed(2)\n",
        "my_first_mlp = nn.Sequential(\n",
        "                nn.Linear(2, 3),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(3, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "ofXYbvCcs55M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass step\n",
        "torch.manual_seed(2)\n",
        "rand_input = random_X\n",
        "output = my_first_mlp(rand_input)\n",
        "print('final output: ', output.data)"
      ],
      "metadata": {
        "id": "aq-ikA7Qw_R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercise#5**\n",
        "Create three Linear layers and connect them in sequence to build an MLP with the following structure:\n",
        "\n",
        "> The first layer has 2 input nodes and 3 output nodes.\n",
        "\n",
        "> The second layer takes 3 input nodes and outputs 6 nodes.\n",
        "\n",
        "> The final layer connects 6 input nodes to 1 output node.\n",
        "\n",
        "> The activation functions should be Sigmoid for each layer (including the final layer)\n",
        "\n",
        "> Put all of your code in a single CoLab cell. Start the code with `torch.manual_seed(2)`\n",
        "\n",
        "> Execute a forward pass step with the initial values; `input_x = torch.tensor( [[-0.5,0.5]] )`"
      ],
      "metadata": {
        "id": "-hkSrqXu1Q25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "du2QLolc16IP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}