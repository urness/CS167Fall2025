{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13721467,"sourceType":"datasetVersion","datasetId":8729933},{"sourceId":13722379,"sourceType":"datasetVersion","datasetId":8730495}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CS167: Day23\n## Kaggle & Fine Tuning CNN Models\n\n#### CS167: Machine Learning, Fall 2025","metadata":{}},{"cell_type":"markdown","source":"## __Put the Model in GPU mode__\n\nWe want to accelerate the training process using graphical processing unit (GPU). You need to enable it (click Settings --> Accelerator--> GPU T4 x2)","metadata":{}},{"cell_type":"code","source":"import torch\n# check GPU (Kaggle will show \"cuda\" if GPU enabled in the notebook settings)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\n\n# Set seeds for reproducibility\nseed = 42  # you can choose any integer\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# If using CUDA:\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if using multi-GPU","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## __Putting Everything Together for AlexNet__","metadata":{}},{"cell_type":"markdown","source":"__Putting Everything Together using our AlexNet Network on our 4-class image recognition Dataset__","metadata":{}},{"cell_type":"code","source":"# ============================================\n# Step 1: imports and device (Kaggle version)\n# ============================================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets, models\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport time\nimport os\n# ===========================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Step 2: dataset paths (Kaggle version)\n# ============================================\n# In Kaggle, uploaded datasets appear under /kaggle/input\n# For example, if your dataset is named \"bike-cat-dog-person\", youâ€™ll see it at:\n# /kaggle/input/bike-cat-dog-person/...\n\n# uncomment for debugging if the files cannot be found\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nbase_dir   = \"/kaggle/input/bike-cat-dog-person\"      # <-- change this name to match your Kaggle dataset\ntrain_dir  = os.path.join(base_dir, \"bcdp_v1/train\")\ntest_dir   = os.path.join(base_dir, \"bcdp_v1/test\")\n\n# uncomment for debugging if the files cannot be found\n#print(\"Train dir:\", train_dir)\n#print(\"Test dir:\", test_dir)\n#print(\"Train subfolders:\", os.listdir(train_dir))\n#print(\"Test subfolders:\", os.listdir(test_dir))\n\n# For AlexNet: normalize with ImageNet mean/std and resize to 227x227\ntransform = transforms.Compose([\n    transforms.Resize((227, 227)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        (0.485, 0.456, 0.406),\n        (0.229, 0.224, 0.225)\n    )\n])\n\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntest_dataset  = datasets.ImageFolder(test_dir,  transform=transform)\n\ndataset_labels = train_dataset.classes\nnumber_of_classes = len(dataset_labels)\nprint(\"Classes:\", dataset_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================\n# Step 3: define Neural Network model here\n# ============================================\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(AlexNet, self).__init__()\n        net = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1 if pretrained else None)\n\n        # retain convolutional and pooling layers\n        self.features = net.features\n        self.avgpool  = net.avgpool\n\n        # replace classifier with new head for our num_classes\n        self.classifier = nn.Sequential(\n            nn.Linear(256 * 6 * 6, 128),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)          # feature extraction from AlexNet\n        x = self.avgpool(x)           # spatial pooling from AlexNet\n        x = torch.flatten(x, 1)       # flatten to (batch_size, feature_dim)\n        x = self.classifier(x)        # MLP for final classification\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 4: training / testing loops \n# ============================================\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    model.train()\n    size = len(dataloader.dataset)\n    running_loss = 0.0\n    correct = 0\n\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # forward + loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # backward + update\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        running_loss += loss.item()\n        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = correct / size\n    return avg_loss, accuracy\n\ndef test_loop(dataloader, model, loss_fn):\n    model.eval()\n    size = len(dataloader.dataset)\n    running_loss = 0.0\n    correct = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n\n            running_loss += loss.item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n            all_preds.append(pred.argmax(1).cpu())\n            all_labels.append(y.cpu())\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = correct / size\n\n    all_preds = torch.cat(all_preds)\n    all_labels = torch.cat(all_labels)\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n\n    return avg_loss, accuracy, conf_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 5: your fine-tuning block (Kaggle-ready)\n# ============================================\ncnn_model = AlexNet(number_of_classes)\ncnn_model.to(device)\nprint(cnn_model)\n\nlearning_rate   = 1e-4\nbatch_size_val  = 32\nepochs          = 10\nloss_fn         = nn.CrossEntropyLoss()\noptimizer       = optim.Adam(cnn_model.parameters(), lr=learning_rate)\nsoftmax         = nn.Softmax(dim=1)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size_val,\n    shuffle=True,\n    num_workers=2,        # Kaggle: use workers to speed up loading\n    pin_memory=True if device == \"cuda\" else False\n)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=batch_size_val,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True if device == \"cuda\" else False\n)\n\ntrain_losses = []\ntest_losses  = []\ntrain_accuracies = []\ntest_accuracies  = []\n\nstart_time = time.time()\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    avg_train_loss, train_accuracy = train_loop(train_dataloader, cnn_model, loss_fn, optimizer)\n    avg_test_loss, test_accuracy, conf_matrix_test = test_loop(test_dataloader, cnn_model, loss_fn)\n\n    train_losses.append(avg_train_loss)\n    test_losses.append(avg_test_loss)\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)\n\n    print(f\"Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}\")\n    print(f\"Test  loss: {avg_test_loss:.4f}, Test  acc: {test_accuracy:.4f}\")\n\nprint(\"AlexNet model has been fine-tuned!\")\ntotal_time_sec = time.time() - start_time\nprint(\"Total fine-tuning time: %.3f sec\" % total_time_sec)\nprint(\"Total fine-tuning time: %.3f hrs\" % (total_time_sec / 3600.0))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualizing the accuracy curves\n\nplt.plot(range(1,epochs+1), train_accuracies)\nplt.plot(range(1,epochs+1), test_accuracies)\nplt.title('Model accuracies after each epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show confusion matrix for final epoch\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_test, display_labels=dataset_labels)\ndisp.plot(xticks_rotation=45,cmap=\"Blues\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now, let's see how the newly-trained model works on 10 sample images from the testing set","metadata":{}},{"cell_type":"code","source":"# ChatGPT generated code\n# Prompt to ChatGPT: https://chatgpt.com/share/6915f698-2ca8-8001-9ba6-38a7314e8b82\n\nimport random\nimport matplotlib.pyplot as plt\nimport torch\n\n# helper to unnormalize images for display (reverse the ImageNet normalization)\ndef unnormalize(img_tensor):\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    return img_tensor * std + mean\n\n# get class names from the dataset object\nclass_names = test_dataset.classes\nprint(\"Class names:\", class_names)\n\n# make sure model is in eval mode\ncnn_model.eval()\n\n# randomly pick 10 indices from the test set\nindices = random.sample(range(len(test_dataset)), 10)\n\nplt.figure(figsize=(15, 10))\n\nwith torch.no_grad():\n    for i, idx in enumerate(indices):\n        img, label = test_dataset[idx]\n\n        # Prepare batch of size 1\n        x = img.unsqueeze(0).to(device)\n\n        # model prediction\n        pred = cnn_model(x)\n        pred_prob = softmax(pred)\n        pred_label = torch.argmax(pred_prob, dim=1).item()\n\n        # check if correct\n        is_correct = (pred_label == label)\n\n        # unnormalize for display\n        disp_img = unnormalize(img).permute(1,2,0).cpu().numpy()\n\n        # plot\n        plt.subplot(2, 5, i+1)\n        plt.imshow(disp_img)\n        plt.axis(\"off\")\n\n        title_color = \"green\" if is_correct else \"red\"\n        plt.title(f\"Pred: {class_names[pred_label]}\\nTrue: {class_names[label]}\",\n                  color=title_color)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}