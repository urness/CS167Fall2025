{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOJhOiGgGLPuwXb3T5/Cki7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day27_Intro_to_Transformers_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day27\n",
        "## Intro to Transformers part 3\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n",
        "\n"
      ],
      "metadata": {
        "id": "TCt5SH6SzJuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "\n",
        "\n",
        "It's not necessary to have GPU for this notebook. However, it won't hurt.\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime (or click on the down arrow near RAM & DISK in upper right)-->Change runtime type-->GPU or TPU_\n",
        "\n",
        "Professor Urness tested this code with the regular CPU option."
      ],
      "metadata": {
        "id": "jSIsZsKfzk4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "We start with setting up the lab by installing the required libraries (`transformers`, `datasets`, and `accelerate`) and ignoring the warnings."
      ],
      "metadata": {
        "id": "3-CDzSn05W55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Fine-Tuning"
      ],
      "metadata": {
        "id": "EPqmtfaHcC5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some necessary import statements:"
      ],
      "metadata": {
        "id": "yLyFOwWQ0EBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "EOS = \"<|endofanswer|>\""
      ],
      "metadata": {
        "id": "fmxGaJ_az_Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establish a small JSON dataset (a knowledge base) with course facts.\n"
      ],
      "metadata": {
        "id": "e9wWfOaovUNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    {\n",
        "        \"question\": \"How many quizzes are in CS 167?\",\n",
        "        \"answer\": \"There are 3 quizzes in CS 167.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is CS 167 about?\",\n",
        "        \"answer\": \"CS 167 is an introduction to machine learning and data science.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How should I prepare for CS 167 quizzes?\",\n",
        "        \"answer\": \"Review lecture notes, complete the practice problems, and understand the concepts, not just the code.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "M0lDphH2mtLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert each JSON object into a simple prompt → completion format.\n",
        "\n",
        "This code uses the `Dataset` package, which is an efficient, standardized container for training data used in machine learning pipelines.\n",
        "\n",
        "*Confession: This also makes our results a little cleaner for this small demonstration*"
      ],
      "metadata": {
        "id": "UJkUpi-2vdXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    f\"Question: {item['question']}\\nAnswer: {item['answer']} {EOS}\"\n",
        "    for item in training_data\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "# dataset will now contain the formatted versions of the \"facts\" provided earlier"
      ],
      "metadata": {
        "id": "GRdhJ_Vimvqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can load the (rather small) model and tokenizer"
      ],
      "metadata": {
        "id": "PGfb_P67wPxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# credit: part of this code generated with the help of ChatGPT\n",
        "model_name = \"gpt2\"   # rather small transformer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# pad token fix for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [EOS]})\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "Su6Hfjrqnrry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers cannot read or understand text directly. They do not operate on characters, words, or sentences, instead they work on vector embeddings. So, we need to tokenize the dataset we are going to incorporate into the model.\n"
      ],
      "metadata": {
        "id": "ObzkvXKhwa-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=150,\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# For GPT-style fine-tuning\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "4OmZp0MKn2-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up training .. this may take a little time\n",
        "In my experience (with just the CPU, it took 60 seconds)"
      ],
      "metadata": {
        "id": "RePeSzpcw5vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings for fine-tuning GPT-2\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./cs167-gpt2\",     # Where to save the fine-tuned model and checkpoints\n",
        "    num_train_epochs=20,           # Number of full passes through the training data\n",
        "    per_device_train_batch_size=1, # How many samples per training step (small dataset → batch size 1 is fine)\n",
        "    learning_rate=5e-5,            # How quickly the model updates its weights during training\n",
        "    logging_steps=5,               # Print training metrics (loss, etc.) every 5 steps\n",
        "    save_steps=500,                # Save a checkpoint every 500 steps (rare, since dataset is tiny)\n",
        "    save_total_limit=1,            # Keep only the most recent checkpoint to avoid clutter\n",
        "    report_to=\"none\",              # Turn off logging to external tools like WandB or TensorBoard\n",
        ")\n",
        "\n",
        "# Trainer object that handles the training loop, optimization, and batching\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Let's train it!\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "F-wohx9kn6rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask the new model a question!"
      ],
      "metadata": {
        "id": "YVLGqiX21jZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Takes a text prompt and asks the model to generate a continuation.\n",
        "# --------------------------------------------------------\n",
        "def answer(question):\n",
        "    # Build the prompt in the same style the model was fine-tuned on\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "    # Tokenize the prompt so the model can process it\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    model.to(device).eval()  # move model and set eval mode\n",
        "    # Generate the model's answer\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=25,                  # Limit the length of the answer so it doesn't ramble\n",
        "        pad_token_id=tokenizer.eos_token_id, # Required because GPT-2 has no pad token\n",
        "        eos_token_id=tokenizer.eos_token_id, # Stop generation when EOS is reached\n",
        "        do_sample=False,                    # Use greedy decoding → deterministic, no randomness\n",
        "    )\n",
        "\n",
        "    # Convert token IDs back into text\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer portion and remove the custom EOS tag if present\n",
        "    return text.split(\"Answer:\")[-1].split(EOS)[0].strip()\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Example usage\n",
        "# --------------------------------------------------------\n",
        "\n",
        "question = \"How many quizzes are in CS 167?\"\n",
        "answer_text = answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer_text}\\n\")\n",
        "\n",
        "# question = \"What is CS 167 about?\"\n",
        "# answer_text = answer(question)\n",
        "# print(f\"Q: {question}\\nA: {answer_text}\\n\")\n",
        "\n",
        "# question = \"How should I prepare for CS 167 quizzes?\"\n",
        "# answer_text = answer(question)\n",
        "# print(f\"Q: {question}\\nA: {answer_text}\\n\")"
      ],
      "metadata": {
        "id": "Lq04QS62sXz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Exercise\n",
        "Change the prompt.\n",
        "1. What kinds of questions can the model get correct?\n",
        "2. What kinds of questions will the model get incorrect?\n",
        "3. Why, do you think, this is happening?"
      ],
      "metadata": {
        "id": "-joh0uGDTJBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "Ii_xfrV1AEVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some necessary import statements"
      ],
      "metadata": {
        "id": "mpoUlhF9ALJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch scikit-learn\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "id2nDHwL87P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establish a small JSON dataset (a knowledge base) with course facts."
      ],
      "metadata": {
        "id": "3kkW9gGYAS0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    {\n",
        "        \"question\": \"How many quizzes are in CS 167?\",\n",
        "        \"answer\": \"There are 3 quizzes in CS 167.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is CS 167 about?\",\n",
        "        \"answer\": \"CS 167 is an introduction to machine learning and data science.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How should I prepare for CS 167 quizzes?\",\n",
        "        \"answer\": \"Review lecture notes, complete the practice problems, and make sure you understand the concepts, not just the code.\"\n",
        "    }\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "1koQwaN8ASbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF = Term Frequency × Inverse Document Frequency\n",
        "\n",
        "The following code:\n",
        "- Splits text into words/features.\n",
        "- Creates a dictionary of all unique words in your documents.\n",
        "- Createas a TF-IDF matrix, which will help emphasize words that carry meaning\n"
      ],
      "metadata": {
        "id": "bqd0WqCFAwBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [item[\"question\"] for item in training_data]\n",
        "vectorizer = TfidfVectorizer()\n",
        "doc_vectors = vectorizer.fit_transform(questions)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Function: retrieve_context\n",
        "# Given a user question, return the top-k most similar Q&A entries.\n",
        "# ----------------------------------------------------------\n",
        "def retrieve_context(user_question, k=2):\n",
        "    q_vec = vectorizer.transform([user_question])\n",
        "    sims = cosine_similarity(q_vec, doc_vectors)[0]\n",
        "    top_indices = sims.argsort()[::-1][:k]\n",
        "    return [training_data[i] for i in top_indices]"
      ],
      "metadata": {
        "id": "JdHe3BN4AoLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can load the (very small) model and tokenizer"
      ],
      "metadata": {
        "id": "Nt8oK_AeBxry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a model. Note that the google/flan-t5-small is good at question-answer\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "_YXuqUYVCciY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, build a prompt, and put the most appropriate elements from the knowledge base into the context of the prompt.\n"
      ],
      "metadata": {
        "id": "aDyUFwCJCmg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a prompt by inserting the retrieved Q&A context and the user's question in a structured format.\n",
        "def build_prompt(user_question, retrieved_items):\n",
        "    context_lines = []\n",
        "    for item in retrieved_items:\n",
        "        context_lines.append(f\"Q: {item['question']}\\nA: {item['answer']}\")\n",
        "    context_text = \"\\n\\n\".join(context_lines)\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a helpful teaching assistant for CS 167 at Drake University.\\n\"\n",
        "        \"Use ONLY the context to answer the student's question.\\n\"\n",
        "        \"If the answer is in the context, copy it or paraphrase it.\\n\"\n",
        "        \"If you don't know, say you don't know.\\n\\n\"\n",
        "        f\"Context:\\n{context_text}\\n\\n\"\n",
        "        f\"Student question: {user_question}\\n\"\n",
        "        \"Answer in one short sentence.\"\n",
        "    )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "Z6DvmZVcA0Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the full RAG pipeline: retrieve context, build the prompt, generate an answer.\n",
        "def rag_answer(user_question, max_new_tokens=32):\n",
        "    retrieved = retrieve_context(user_question)\n",
        "    prompt = build_prompt(user_question, retrieved)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return answer, retrieved, prompt"
      ],
      "metadata": {
        "id": "NWdadRaWDKG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try it out!!"
      ],
      "metadata": {
        "id": "rc8pBLM6DVrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many quizzes are in CS 167?\"\n",
        "answer_text, retrieved, prompt = rag_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer_text}\\n\")\n",
        "\n",
        "# question = \"What is CS 167 about?\"\n",
        "# answer_text, retrieved, prompt = rag_answer(question)\n",
        "# print(f\"Q: {question}\\nA: {answer_text}\\n\")\n",
        "\n",
        "# question = \"How should I prepare for CS 167 quizzes?\"\n",
        "# answer_text, retrieved, prompt = rag_answer(question)\n",
        "# print(f\"Q: {question}\\nA: {answer_text}\\n\")"
      ],
      "metadata": {
        "id": "E9XauINaDOdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Exercise: Change the prompt.\n",
        "1. What kinds of questions can the RAG model get correct?\n",
        "2. What kinds of questions will the RAG model get incorrect?\n",
        "3. Compare RAG with Fine-Tuning"
      ],
      "metadata": {
        "id": "2VC9amCfd-S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following prints out the context that was used to construct the answer:\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "Qq5EdVX6Dp-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Just a regular model.."
      ],
      "metadata": {
        "id": "R59hcehIeOWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will **not** utilize the knowledge base before answering -- it just uses the model. Does it still get the answer correct?"
      ],
      "metadata": {
        "id": "kOKAhs_VDZQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_without_kb(user_question, max_new_tokens=32):\n",
        "    \"\"\"\n",
        "    Ask model directly, no retrieved context.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are a helpful teaching assistant for CS 167 at Drake University.\\n\"\n",
        "        \"Answer the following question based on your general knowledge.\\n\\n\"\n",
        "        f\"Question: {user_question}\\n\"\n",
        "        \"Answer in one short sentence.\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "question = \"How many quizzes are in CS 167?\"\n",
        "answer_text = answer_without_kb(question)\n",
        "print(f\"Q: {question}\\nA: {answer_text}\\n\")\n",
        "\n",
        "# question = \"What is CS 167 about?\"\n",
        "# answer_text = answer_without_kb(question)\n",
        "# print(f\"Q: {question}\\nA: {answer_text}\\n\")\n"
      ],
      "metadata": {
        "id": "5FHQI8YP_NpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}