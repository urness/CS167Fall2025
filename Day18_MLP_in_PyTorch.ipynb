{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+qOF/1Dlp8rfvJoqK7qSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day18_MLP_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day18\n",
        "## Building a Simple MLP using PyTorch Library\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n"
      ],
      "metadata": {
        "id": "8TmDLmOfqmAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "We can use PyTorch Framework to build and train MLPs and other neural networks such as CNN, RNN, Transformers. Let's learn the basics of PyTorch."
      ],
      "metadata": {
        "id": "9r7g0RAbrLTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "v1CBrgItrNzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime-->Change runtime type-->GPU or TPU_"
      ],
      "metadata": {
        "id": "3awNnb-YtDpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if torch.cuda is available, otherwise it will use CPU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "# if it prints 'cuda' then colab is running using GPU device"
      ],
      "metadata": {
        "id": "7KRrGdHZtKK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Building Multilayer Perceptron (MLP)__\n",
        "\n",
        "A multilayer perceptron is the simplest type of neural network. It consists of perceptrons (aka nodes, neurons) arranged in layers.\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_example.png\" width=800/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "i-nIlQGRtOlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate 4 random samples of (x1, x2) for the above network\n",
        "torch.manual_seed(0)                      # for reproducibility\n",
        "random_X = torch.randn(4,2)               # you could imagine that these are pairs of (x1, x2) as shown in the above table\n",
        "print('random_X = \\n', random_X.numpy())\n",
        "\n",
        "\n",
        "input_feature_size = random_X.shape[1]    # number of columns corresponds to feature dimension\n",
        "print('\\n\\ninput feature dimension: ', input_feature_size)"
      ],
      "metadata": {
        "id": "oz45qBAstX3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of these questions need to be answered before you set up your neural network:\n",
        "- Q1: how many hidden layers should be there? (depth)\n",
        "- Q2: how many neurons should be in each layer? (width)\n",
        "- Q3:  how many dense connections should be there in between each adjacent layers\n",
        "- Q4: what should the activation be at each of the intermediate layers?\n",
        "  - we could use _sigmoid()_, _tanh()_, _rectified-linear-unit()_, etc\n",
        "- Q5: what should be activation of the final layer\n",
        "  - depends the task _classification_ (sigmoid(), softmax()) vs. _regression_"
      ],
      "metadata": {
        "id": "fppiWX3yvF6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1) # for reproducibility\n",
        "# Q1: how many hidden layers should be there? (depth)\n",
        "# answer: there is only 1 hidden layer\n",
        "num_of_hidden_layer = 1\n",
        "\n",
        "# Q2: how many neurons should be in each layer? (width)\n",
        "# answer: there are 2 neurons in the input  layer\n",
        "#         there are 3 neurons in the hidden layer\n",
        "#         there are 1 neurons in the output layer\n",
        "#num_of_neurons_input_layer  = input_feature_size # also can be assigned from 'input_feature_size' (which we computed in the previous cell)\n",
        "num_of_neurons_input_layer  = 2\n",
        "num_of_neurons_hidden_layer = 3\n",
        "num_of_neurons_output_layer = 1\n",
        "\n",
        "# Q3 how many dense connections should be there in between each adjacent layers\n",
        "# answer: there should be 2x3 dense connnections (between input  layer and hidden layer: dense_connections_W1)\n",
        "#         there should be 3x1 dense connnections (between hidden layer and output layer: dense_connections_W2)\n",
        "dense_connections_W1 = torch.randn(num_of_neurons_input_layer,  num_of_neurons_hidden_layer)\n",
        "dense_connections_W2 = torch.randn(num_of_neurons_hidden_layer, num_of_neurons_output_layer)\n",
        "print('Random initialized weights between input  layer and hidden layer: dense_connections_W1=\\n', dense_connections_W1.numpy())\n",
        "print('Random initialized weights between input  layer and hidden layer: dense_connections_W2=\\n', dense_connections_W2.numpy())\n",
        "# add the bias terms for all the layers except input layer\n",
        "bias_terms_hidden    = torch.randn(num_of_neurons_hidden_layer)\n",
        "bias_terms_output    = torch.randn(num_of_neurons_output_layer)\n",
        "print('bias_terms_hidden:\\n', bias_terms_hidden.numpy())\n",
        "print('bias_terms_output:\\n', bias_terms_output.numpy())"
      ],
      "metadata": {
        "id": "0Vfupeo7vP9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multilayer perceptron is the simplest type of neural network. It  consists of perceptrons (aka nodes, neurons) arranged in layers. There are 6 connections between input and hidden layer and 3 connections between hidden and output layers with the random initialized using PyTorch code above.\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_example0.png\" width=800/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "-_5SJidswvu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: what should the activation be at each of the intermediate layers?\n",
        "# answer: let use sigmoid() activation function in the hidden layer\n",
        "sigmoid_activation_hidden = nn.Sigmoid()"
      ],
      "metadata": {
        "id": "OmY2ynmTxtaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: what should be activation of the final layer (let's assume we are using a binary classification task for which sigmoid ctivation is used)\n",
        "sigmoid_activation_output = nn.Sigmoid()"
      ],
      "metadata": {
        "id": "9p48__u0xvos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Forward Pass in Multilayer Perceptron (MLP)__\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_example_forward_pass1.png\" width=800/>\n",
        "</div>\n",
        "\n",
        "Each neuron contains two operations:\n",
        "- a dot product between a weight vector (edges in the graph) and an input vector\n",
        "- that number through an activation function, which produces a number as an output\n",
        "\n",
        "We can collective do all these dot products in a single layer using a single matrix-matrix multiplication [torch.matmul()](https://pytorch.org/docs/stable/generated/torch.matmul.html) as follows.\n",
        "\n",
        "Also add the bias-term after computing the matrix multiplication"
      ],
      "metadata": {
        "id": "nbYUS7Aux5du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_mult_X_and_W1 = torch.matmul(random_X[0,:], dense_connections_W1) + bias_terms_hidden\n",
        "print('hidden layer input vector and weight vector dot products: \\n', matrix_mult_X_and_W1.numpy())\n",
        "output_hidden_layer = sigmoid_activation_hidden(matrix_mult_X_and_W1)\n",
        "print('output of hidden layer: \\n', output_hidden_layer.numpy())"
      ],
      "metadata": {
        "id": "WfU1BA9Hz_Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Layer Calculations"
      ],
      "metadata": {
        "id": "nX5b6ICj2sl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_mult_hidden_and_W2 = torch.matmul(output_hidden_layer, dense_connections_W2) + bias_terms_output\n",
        "print('output of output layer: \\n', matrix_mult_hidden_and_W2)\n",
        "final_output = sigmoid_activation_output(matrix_mult_hidden_and_W2)\n",
        "print('output of hidden layer: \\n', final_output.numpy())"
      ],
      "metadata": {
        "id": "j7sB5NWU01GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Group activity__#\n",
        "Make another simple MLP with the specifications below and perform the 'Forward Pass' of the MLP."
      ],
      "metadata": {
        "id": "Ydjmn67Q3h5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0) # for reproducibility\n",
        "# Q1: how many hidden layers should be there? (depth)\n",
        "# answer: there is only 1 hidden layer\n",
        "num_of_hidden_layer = 1\n",
        "\n",
        "\n",
        "# Q2: how many neurons should be in each layer? (width)\n",
        "# answer: there are 3 neurons in the input  layer\n",
        "#         there are 4 neurons in the hidden layer\n",
        "#         there are 1 neurons in the output layer\n",
        "num_of_neurons_input_layer  =\n",
        "#num_of_neurons_input_layer  = input_feature_size # also can be assigned from 'input_feature_size' (which we computed in the previous cell)\n",
        "num_of_neurons_hidden_layer =\n",
        "num_of_neurons_output_layer =\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Q3 how many dense connections should be there in between each adjacent layers\n",
        "# answer: there should be ?x? dense connnections (between input  layer and hidden layer: dense_connections_W1)\n",
        "#         there should be ?x1 dense connnections (between hidden layer and output layer: dense_connections_W2)\n",
        "# add the bias terms for all the layers except input layer\n",
        "\n",
        "\n",
        "# Q4: what should the activation be at each of the intermediate layers?\n",
        "# answer: let use sigmoid() activation function in the hidden layer\n",
        "\n",
        "# Q5: what should be activation of the final layer (let's assume we are using a binary classification task for which sigmoid activation is used)\n",
        "\n",
        "\n",
        "# do the Forward Pass in Multilayer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "GhtxxDoZ3ev6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}