{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8kzFNa9XlNEDPRUcgZcmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day25_Intro_to_Transformers_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day25\n",
        "## Intro to Transformers part 2\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n",
        "\n"
      ],
      "metadata": {
        "id": "TCt5SH6SzJuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Credit__:\n",
        "\n",
        "Much of the code and lecture materials used from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "Free online course: [How Transformers Work](https://learn.deeplearning.ai/courses/how-transformer-llms-work)\n"
      ],
      "metadata": {
        "id": "B7e3F5trzPV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "\n",
        "\n",
        "It's not necessary to have GPU for this notebook. However, it won't hurt.\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime (or click on the down arrow near RAM & DISK in upper right)-->Change runtime type-->GPU or TPU_\n",
        "\n",
        "Professor Urness tested this code with the GPU option: T4"
      ],
      "metadata": {
        "id": "jSIsZsKfzk4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lesson, you will reinforce your understanding of the transformer architecture by exploring the decoder-only [model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) `microsoft/Phi-3-mini-4k-instruct`."
      ],
      "metadata": {
        "id": "pPPsAfPS5O9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "We start with setting up the lab by installing the required libraries (`transformers` and `accelerate`) and ignoring the warnings."
      ],
      "metadata": {
        "id": "3-CDzSn05W55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some necessary import statements:"
      ],
      "metadata": {
        "id": "yLyFOwWQ0EBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers>=4.41.2 accelerate>=0.31.0\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "fmxGaJ_az_Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the LLM"
      ],
      "metadata": {
        "id": "lC-2TRIx5_G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first load the model and its tokenizer. For that you will first import the classes: `AutoModelForCausalLM` and `AutoTokenizer`. When you want to process a sentence, you can apply the tokenizer first and then the model in two separate steps. Or you can create a pipeline object that wraps the two steps and then apply the pipeline to the sentence. You'll explore both approaches in this notebook. This is why you'll also import the `pipeline` class."
      ],
      "metadata": {
        "id": "Mc02Y9lH6BMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the required classes\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed"
      ],
      "metadata": {
        "id": "j7v4p3Rc6TeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"  # 124M parameters\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "AQVyjL0b6WTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can wrap the model and the tokenizer in a [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline) object that has \"text-generation\" as task."
      ],
      "metadata": {
        "id": "qL1y-Ob-6_cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # False means to not include the prompt text in the returned text\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False\n",
        "    )"
      ],
      "metadata": {
        "id": "pJMqS3wy7Ci6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating a Text Response to a Prompt"
      ],
      "metadata": {
        "id": "1poN5dxg8ek0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll now use the pipeline object (labeled as generator) to generate a response consisting of 50 tokens to the given prompt."
      ],
      "metadata": {
        "id": "odQs2GUS8hoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. \"\n",
        "output = generator(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "vmZjI2ft8kcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Model's Architecture"
      ],
      "metadata": {
        "id": "Sada_G-h_sXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can print the model to take a look at its architecture.\n"
      ],
      "metadata": {
        "id": "FlWkHJNj_vUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "1nGkPIr8_xTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary size is 50257 tokens, and the size of the vector embedding for each token is 768."
      ],
      "metadata": {
        "id": "FCBKRzf8_8mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer.wte"
      ],
      "metadata": {
        "id": "yjB5tTt1AHOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 50257 is the vocabulary size; GPT-2 has 50,257 unique tokens in its tokenizer\n",
        "- 768 is the model dimension — each token is represented as a 768-dimensional vector in GPT-2 “small”"
      ],
      "metadata": {
        "id": "E8rvmwJQAxiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer.wpe"
      ],
      "metadata": {
        "id": "TkvZBHyoAcam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1024\tis the maximum context length (max sequence of tokens GPT-2 can “see”)\n",
        "- 768\tagain, the model dimension"
      ],
      "metadata": {
        "id": "lPMJcDPEBb7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer.h"
      ],
      "metadata": {
        "id": "ECG6rAn8DdUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of transformer layers: 12"
      ],
      "metadata": {
        "id": "AlSiDfJtDu0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "Load the model 'gpt2-large'\n",
        "print out the model tranformer specifications, as above.\n",
        "What is\n",
        "1. The vocabulary size\n",
        "2. The model dimension\n",
        "3. The maximum context length\n",
        "4. The number of layers of transformers"
      ],
      "metadata": {
        "id": "ETZcAI4jBiUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "_vaHMNJ1B10m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model\n"
      ],
      "metadata": {
        "id": "5ZAAv6boB7IN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}