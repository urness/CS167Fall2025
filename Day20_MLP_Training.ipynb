{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMihyY7iqCbGBnNdIc5wdv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day20_MLP_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day20\n",
        "## MLP Training\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n"
      ],
      "metadata": {
        "id": "8TmDLmOfqmAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Do This: Apply for access to Colab Pro for Education**\n",
        "\n",
        "1. Open Colab\n",
        "2. Settings --> Select \"Colab Pro\"\n",
        "3. Click on \"Learn more\" button\n",
        "4. Click on \"No cost for students and educators\" button\n",
        "5. Fill out form.\n",
        "6. Verify in email\n",
        "\n",
        "**What do Colab Pro for Education do for you?**\n",
        "\n",
        "- It gives you the same features as a paid Colab Pro subscription for one year free of charge.\n",
        "- Better compute resources compared to the free tier: access to more powerful GPUs, more memory, and longer session runtimes.\n"
      ],
      "metadata": {
        "id": "7BNn3itOVJuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "\n",
        "*Not **Manditory** for today, but it can save you a few moments of computation time*\n",
        "\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime (or click on the down arrow near RAM & DISK in upper right)-->Change runtime type-->GPU or TPU_"
      ],
      "metadata": {
        "id": "3awNnb-YtDpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if torch.cuda is available, otherwise it will use CPU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "# if it prints 'cuda' then colab is running using GPU device"
      ],
      "metadata": {
        "id": "7KRrGdHZtKK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PjU46rJrpQZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Review**"
      ],
      "metadata": {
        "id": "5qzyilKOM481"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Linear Layers using PyTorch"
      ],
      "metadata": {
        "id": "fPGYgJT1pfRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create input values\n",
        "torch.manual_seed(2) # for reproducibility (you will get the same random number every time you run this cell)\n",
        "\n",
        "number_of_samples = 1\n",
        "random_X = torch.randn(number_of_samples, 2) # two X values"
      ],
      "metadata": {
        "id": "LhpZlUGPjqSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's build the simple 1-hidden layer feedforward neural network!**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_examle_wo_weights.png\" width=400/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8jNPsQTlsyjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creation of our network\n",
        "torch.manual_seed(2)\n",
        "my_first_mlp = nn.Sequential(\n",
        "                nn.Linear(2, 3),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(3, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "ofXYbvCcs55M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass step\n",
        "rand_input = random_X\n",
        "output = my_first_mlp(rand_input)\n",
        "print('final output: ', output.data)"
      ],
      "metadata": {
        "id": "aq-ikA7Qw_R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PWLFiPIVKVA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Introducing creating a custom PyTorch Network class**\n",
        "\n",
        "A multilayer perceptron is the simplest type of neural network. It consists of perceptrons (aka nodes, neurons) arranged in layers.\n",
        "Create a network class with two methods:\n",
        "- _init()_\n",
        "- _forward()_\n"
      ],
      "metadata": {
        "id": "zCXUZwgAM7TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# You can give any name to your new network, e.g., SimpleMLP.\n",
        "# However, you have to mandatorily inherit from nn.Module to\n",
        "# create your own network class. That way, you can access a lot of\n",
        "# useful methods and attributes from the parent class nn.Module\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "    # ...\n",
        "    # ...\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for MLP forward pass should take place here\n",
        "    # ...\n",
        "    # ...\n",
        "    return x"
      ],
      "metadata": {
        "id": "AbkIQltaNADu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Now, using a custom Pytorch Network class**"
      ],
      "metadata": {
        "id": "Oc2ZlFvTKWxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's build the simple 1-hidden layer feedforward neural network!**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_examle_wo_weights.png\" width=400/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "TSPXLTZlPE0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "    self.network_layers = nn.Sequential(\n",
        "                nn.Linear(2, 3),    # input layer: 2 features → 3 neurons\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(3, 1)      # output layer: 3 → 1 output\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for MLP forward pass should take place here\n",
        "    output = self.network_layers(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "r2wcOzqRKrIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2)\n",
        "\n",
        "# Create an instance\n",
        "model = SimpleMLP()\n",
        "\n",
        "# Forward pass\n",
        "output = model(random_X)\n",
        "print('output: ', output.data)"
      ],
      "metadata": {
        "id": "FRAbvN8PL4WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ah8GybEUMp3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Building Modular Code for Multilayer Perceptron (MLP)__\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_network1.png\" width=800/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "uQCCuwpSQukK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the MLP as shown in the picture above using this template. In general, we will follow this template for constructing other neural networks such as CNN, RNN, and Transformer in PyTorch. Hence, it is a very generic setup. Here are the useful PyTorch modules we will be using for MLP construction:\n",
        "- [nn.Linear()](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "  - creates the dense connections (corresponding to the weights of edges) between two adjacent layers (_left layer_ and _right layer_)\n",
        "  - just provide __#neurons_left_layer__ and __#neurons_right_layer__\n",
        "- [nn.Sigmoid()](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#sigmoid)\n",
        "- [nn.ReLU()](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu)\n",
        "- [nn.Softmax()](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax)"
      ],
      "metadata": {
        "id": "6_SQgmo7QpTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test a forward pass of our MLP using one of the training samples. You need to convert a matrix of numbers into a contiguous vector using the following PyTorch module:\n",
        "- [nn.flatten()](https://docs.pytorch.org/docs/stable/generated/torch.flatten.html)\n"
      ],
      "metadata": {
        "id": "1VbsApY_s6mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pdb\n",
        "\n",
        "class SimpleMLPv1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # your network layer construction should take place here\n",
        "    self.network_layers = nn.Sequential(\n",
        "                nn.Linear(784, 256),  # linear transformation module (input=784, output=256)\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 10) # linear transformation module (input=256, output=10)\n",
        "                              # usually this number should be equal to the total number of classes in your classification task\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for MLP forward pass should take place here\n",
        "    x = self.flatten(x)\n",
        "    output = self.network_layers(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Xnlr3TyRQzac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Another model with 2 hidden layers**"
      ],
      "metadata": {
        "id": "bWXNWjjlRgT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pdb\n",
        "\n",
        "class SimpleMLPv2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.network_layers = nn.Sequential(\n",
        "                nn.Linear(784, 512),  # linear transformation module (input=784, output=512)\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),  # linear transformation module (input=512, output=256)\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 10) # linear transformation module (input=256, output=10)\n",
        "                              # usually this number should be equal to the total number of classes in your classification task\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    output = self.network_layers(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "NjZRUuVWRokt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the structure of your MLP\n",
        "mlp_model = SimpleMLPv2()\n",
        "print(mlp_model)"
      ],
      "metadata": {
        "id": "eOJ1NGSgRwS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sizes of weights and biases of your MLP's 1st hidden layers\n",
        "\n",
        "# Access the first Linear layer (input → 512)\n",
        "first_linear = mlp_model.network_layers[0]\n",
        "\n",
        "print(\"Weights shape:\", first_linear.weight.shape)\n",
        "print(\"Biases shape:\", first_linear.bias.shape)"
      ],
      "metadata": {
        "id": "fZmY4dg9RzQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the randomly initialized values of weights and biases of your MLP's 1st hidden layers\n",
        "print('weights of first_hidden_layer: \\n ', first_linear.weight)\n",
        "print('bias of first_hidden_layer: \\n ', first_linear.bias)"
      ],
      "metadata": {
        "id": "bNZ_4sbtR1e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sizes of weights and biases of your MLP's 2nd hidden layer\n",
        "# Access the first Linear layer (512 → 256)\n",
        "second_linear = mlp_model.network_layers[2]\n",
        "print(\"Weights shape:\", second_linear.weight.shape)\n",
        "print(\"Biases shape:\", second_linear.bias.shape)"
      ],
      "metadata": {
        "id": "PhJ67feJR42w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sizes of weights and biases of your MLP's 3rd layer\n",
        "# Access the first Linear layer (256 → 10)\n",
        "third_linear = mlp_model.network_layers[4]\n",
        "print(\"Weights shape:\", third_linear.weight.shape)\n",
        "print(\"Biases shape:\", third_linear.bias.shape)"
      ],
      "metadata": {
        "id": "S8by9DSnFchQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercise -- Create a model SimpleMLPv3 with 3 hidden layers\n",
        "# Input layer has 784 inputs\n",
        "# First layer has 256 nodes\n",
        "# Second layer has 128 nodes\n",
        "# Third layer has 64 nodes\n",
        "# Output layer has 10\n"
      ],
      "metadata": {
        "id": "_EzCuzmEjtDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Load the Dataset for your MLP__"
      ],
      "metadata": {
        "id": "C0WnqIHFSCz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily import some [built-in datasets](https://docs.pytorch.org/vision/main/datasets.html) from PyTorch's [torchvision.datasets](torchvision.datasets) module\n",
        "- [MNIST](https://en.wikipedia.org/wiki/MNIST_database)\n",
        "  - each image size: 28x28 grayscale image\n",
        "  - each image is associated with a label from __10 classes__\n",
        "  - training set of 60,000 examples and a test set of 10,000 examples\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b1/MNIST_dataset_example.png\" width=500/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Qe3b7VVRSFq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "350np-1zSe8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "# torchvision has many deep learning benchmark datasets MNIST, CIFAR-10, Caltech-50, etc\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/CS167/datasets\", # headsup! You can replace this path so that it points to a directory in your Google Drive\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor() # specify the feature and label transformations\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/CS167/datasets\", # headsup! You can replace this path so that it points to a directory in your Google Drive\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n"
      ],
      "metadata": {
        "id": "_SZVgjwbSme8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that the files are downloaded, let's look at an image from the training set\n",
        "\n",
        "## note you can access any of the 60,000 training images\n",
        "image, label = training_data[41] # I chose 41, just for fun -- try some others\n",
        "print(image.shape, label)\n",
        "\n",
        "plt.figure(figsize=(2, 2))  # small figure to avoid upscaling\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\", interpolation=\"none\")\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jbsx3OP1UalX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Prepare Your Data with DataLoader for Training/Testing__\n",
        "We just explored one sample of data at a time. As we have seen in our discussion of the optimizer, specifically __Stochastic Gradient Descent (SGD)__, during training your network, we may need to pass them in __minibatches__. PyTorch has a module called __DataLoader__, which will do this automatically for us as long as we provide the right arguments:\n",
        "- prepare the __minibatches__ with the given _batch_size_ eg 16, 32, 64, 128, etc\n",
        "- multiprocessing to speed up the data retrieval\n",
        "- reshuffle the data at every __epoch__\n"
      ],
      "metadata": {
        "id": "BNjQz3jNWnPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#                             pairs of items,    minibatch size,        random shuffling turned ON\n",
        "train_dataloader = DataLoader(training_data,     batch_size=128,        shuffle=True)\n",
        "test_dataloader  = DataLoader(test_data,         batch_size=128,        shuffle=False) # for testing/inference: it is not necessary to shuffle\n"
      ],
      "metadata": {
        "id": "vSsmlusYWwf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take it one batch at a time..."
      ],
      "metadata": {
        "id": "8YwA9A94YOWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# explore the data from the train_dataloader\n",
        "train_inputs, train_labels = next(iter(train_dataloader)) # returns a batch of 128 train-images and train-labels\n",
        "\n",
        "print(f\"Images batch shape: {train_inputs.shape}\")\n",
        "print(f\"Labels batch shape: {train_labels.shape}\")\n",
        "\n",
        "#display some more examples from the MNIST dataset from this batch\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "cols, rows = 5, 2\n",
        "\n",
        "for i in range(cols * rows):\n",
        "    img= train_inputs[i]\n",
        "    label = train_labels[i]\n",
        "    figure.add_subplot(rows, cols, i+1)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TLVCjervmXnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Forward Pass using your Dataset and your MLP__\n",
        "\n",
        "The forward method inside our network class, __SimpleMLPv2__, will be invoked if we provide an input tensor __'X'__ to the network object we instantiated earlier, i.e., __mlp_model__, as follows:\n",
        "- _output = mlp_model(X)_\n",
        "\n",
        "Finally, we convert the ouput from the model into probabilities using __Softmax()__ module:\n",
        "- _predicted_probability = softmax_activation(output)_\n"
      ],
      "metadata": {
        "id": "TaTTiKr8W9lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "remember ... we've set up a network in the code above (mlpmodel -- via the class SimpleMLPv2) to take in 784 inputs (28x28 pixel values) and output 10 values ... but we haven't trained it yet.... But, we can still put an image through the network and see what it will predict."
      ],
      "metadata": {
        "id": "LHr1TxkyXooo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img   = train_inputs[127]                 # I picked the sample at the 127 index, but you can pick any index in between 0 to batch_size-1=127\n",
        "label = train_labels[127]\n",
        "\n",
        "softmax_activation = nn.Softmax(dim=1)\n",
        "\n",
        "# Load up the model\n",
        "mlp_model = SimpleMLPv2()\n",
        "\n",
        "# data and model should be placed to the same device (either GPU or CPU)\n",
        "X = img.unsqueeze(0).to(device)         # sending the data tensor to GPU (if available)\n",
        "mlp_model.to(device)                      # sending the model to GPU (if available) print(f\"device {device} and model: \\n {mlp_model}\")\n",
        "output = mlp_model(X)                     # last layer of our network will return 10 values each will range in between in [-infty, infty]\n",
        "\n",
        "predicted_probability = softmax_activation(output)  # these raw numbers scaled to values [0, 1] representing the model’s predicted probabilities for each class\n",
        "\n",
        "print('predited probability \\n', predicted_probability)\n",
        "y_pred = predicted_probability.argmax()\n",
        "print(f\"Predicted class: {y_pred}\")\n",
        "print(f\"Actual class: {label}\")\n"
      ],
      "metadata": {
        "id": "v12CMiP0YAuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Defining Loss function__\n",
        "\n",
        "- [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
        "  - useful when training a __classification problem__ with __C__ classes.\n",
        "  - criterion computes the cross entropy loss between input logits (raw scores before softmax) and target\n",
        "- [nn.MSELoss()](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
        "  - useful when training a __regression problem__\n",
        "  - criterion that measures the mean squared error (squared L2 norm) between each element in the input _x_ and target _y_\n"
      ],
      "metadata": {
        "id": "uxMujpCqaEzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss() # this is useful for multiclass classification task"
      ],
      "metadata": {
        "id": "M75ITMOha0jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Initializing the Optimizer__\n",
        "\n",
        "Optimiztaion, as we have discussed earlier, is process of adjusting model parameters to reduce model error in each training step.\n",
        "\n",
        "PyTorch provides a selection of optimization algorithms in the [torch.optim](https://pytorch.org/docs/stable/optim.html) package. Some of them are as follows:\n",
        "- [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
        "- [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
        "- [torch.optim.RMSprop](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop)\n",
        "\n",
        "In addition to selecting the optimizer, we can also select the hyperparameters which are referred to as *adjustable parameters* crucial for controlling the model optimization process. You can influence the training and convergence of the model by tweaking these hyperparameters:\n",
        "- __epochs:__ denotes the number of iterations over the dataset\n",
        "- __batch size:__ represents the quantity of data samples in each iteration propagated through the network before updating the parameters\n",
        "- __learning rate:__ determines the extent of parameter updates made at each batch/epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "JnZzJwrIa3gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size    = 128 # If the total sample size is N, setting batch_size=128 will divide the data into N÷128 mini-batches of tensors\n",
        "epochs        = 10\n",
        "# let's use SGD optimization algorithm for training our model\n",
        "optimizer     = torch.optim.SGD(mlp_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "p_sjBOH2bBEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Putting Everything Together MLP__"
      ],
      "metadata": {
        "id": "OZFiLUGCbjho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Putting Everything Together using our SimpleMLPv2 Network on Fashion-MNIST Dataset__\n",
        "\n",
        "(we've set up all of these steps above, exept for step 4 and 5)"
      ],
      "metadata": {
        "id": "WXE_abPFbpMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: load the Torch library and other utilities\n",
        "#----------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import time\n"
      ],
      "metadata": {
        "id": "v45-llRwbYVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: load the dataset, ie, we are experimenting with MNIST\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/CS167/datasets\", # headsup! You can replace this path so that it points to a directory in your Google Drive\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor() # specify the feature and label transformations\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/CS167/datasets\", # headsup! You can replace this path so that it points to a directory in your Google Drive\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n"
      ],
      "metadata": {
        "id": "eyZ3mpUQbjwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create your MLP Network (this is just copied from above)\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import pdb\n",
        "\n",
        "class SimpleMLPv2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.network_layers = nn.Sequential(\n",
        "                nn.Linear(784, 512),  # linear transformation module (input=784, output=512)\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),  # linear transformation module (input=512, output=256)\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 10) # linear transformation module (input=256, output=10)\n",
        "                              # usually this number should be equal to the total number of classes in your classification task\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    output = self.network_layers(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "wZZXzKc3bscx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Your training and testing functions\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Executes one full training epoch for the given model.\n",
        "\n",
        "    Iterates over all batches in the provided DataLoader, performing the following steps:\n",
        "    - Moves input and target tensors to the selected device (CPU or GPU)\n",
        "    - Computes predictions and loss for each batch\n",
        "    - Performs backpropagation and optimizer updates\n",
        "    - Tracks and prints training loss periodically\n",
        "\n",
        "    Args:\n",
        "        dataloader (torch.utils.data.DataLoader):\n",
        "            The DataLoader providing batches of training data (inputs and labels).\n",
        "        model (torch.nn.Module):\n",
        "            The neural network model to be trained.\n",
        "        loss_fn (torch.nn.Module or callable):\n",
        "            The loss function used to compute the training loss.\n",
        "        optimizer (torch.optim.Optimizer):\n",
        "            The optimizer responsible for updating the model’s parameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The average training loss across all batches in this epoch.\n",
        "    \"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()                   # set the model to training mode for best practices\n",
        "\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # compute prediction and loss\n",
        "        X = X.to(device)                  # send data to the GPU device (if available)\n",
        "        y = y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()      # compute gradients\n",
        "        optimizer.step()     # apply updates\n",
        "        optimizer.zero_grad()# clear old gradients\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return train_loss/len(dataloader)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    \"\"\"\n",
        "    Evaluates the model’s performance on a test (or validation) dataset.\n",
        "\n",
        "    Runs a forward pass over all batches in the provided DataLoader with gradient\n",
        "    computation disabled, accumulating loss and accuracy metrics.\n",
        "\n",
        "    Args:\n",
        "        dataloader (torch.utils.data.DataLoader):\n",
        "            The DataLoader providing batches of test or validation data.\n",
        "        model (torch.nn.Module):\n",
        "            The trained neural network model to evaluate.\n",
        "        loss_fn (torch.nn.Module or callable):\n",
        "            The loss function used to compute the evaluation loss.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss over all test batches.\n",
        "\n",
        "    Prints:\n",
        "        Accuracy (% of correct predictions) and average test loss.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()                    # set the model to evaluation mode for best practices\n",
        "\n",
        "    size        = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "\n",
        "            X = X.to(device)                     # send data to the GPU device (if available)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "HE1kKZpCb616"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: prepare the DataLoader and select your optimizer and set the parameters for learning the model from DataLoader\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "mlp_model = SimpleMLPv2() ## model Class name here\n",
        "mlp_model.to(device)      ## device should have been determined earlier (at top of notebook)\n",
        "learning_rate = 1e-3\n",
        "batch_size_val = 64\n",
        "epochs = 20\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size_val)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size_val)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "test_losses  = []\n",
        "start_time   = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    avg_train_loss = train_loop(train_dataloader, mlp_model, loss_fn, optimizer)\n",
        "    avg_test_loss  = test_loop(test_dataloader, mlp_model, loss_fn)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    test_losses.append(avg_test_loss)\n",
        "\n",
        "print(\"Total fine-tuning time: %.3f sec\" %( (time.time()-start_time)) )\n",
        "print(\"Total fine-tuning time: %.3f hrs\" %( (time.time()-start_time)/3600) )\n",
        "\n",
        "print(mlp_model.__class__.__name__, \" model has been trained!\")\n"
      ],
      "metadata": {
        "id": "wUkcia3dbrLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing the loss curves\n",
        "plt.plot(range(1,epochs+1), train_losses)\n",
        "plt.plot(range(1,epochs+1), test_losses)\n",
        "plt.title('Model average losses after each epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dXujFvdZd1nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now with a trained model.... let's see how well it does on a few specific examples:\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "test_dataloader  = DataLoader(test_data,         batch_size=128,        shuffle=False) # for testing/inference: it is not necessary to shuffle\n",
        "# we need to load data a batch at a time -- loading all of the data in memory is not efficient (or even possible sometimes)\n",
        "\n",
        "test_inputs, test_labels = next(iter(test_dataloader)) # returns a batch of 128 train-images and train-labels\n",
        "\n",
        "mlp_model.eval() # puts model into evaluation mode (training = False)\n",
        "images_shown = 12\n",
        "\n",
        "X_batch, y_batch = next(iter(test_dataloader)) # returns a batch of 128 train-images and train-labels\n",
        "X_batch = X_batch.to(device)\n",
        "y_batch = y_batch.to(device)\n",
        "\n",
        "test_inputs, test_labels = next(iter(test_dataloader)) # returns a batch of 128 train-images and train-labels\n",
        "test_inputs = test_inputs.to(device) #make sure we are on the same device (GPU or CPU)\n",
        "test_labels = test_labels.to(device)\n",
        "\n",
        "# run a forward pass -- no need to compute gradients\n",
        "with torch.no_grad():\n",
        "    logits = mlp_model(X_batch)\n",
        "\n",
        "# what are the predictions?\n",
        "preds = logits.argmax(dim=1)\n",
        "\n",
        "# plot values in a grid\n",
        "plt.figure(figsize=(10,6))\n",
        "for i in range(images_shown):\n",
        "    ax = plt.subplot(3, 4, i+1)\n",
        "    plt.imshow(test_inputs[i].cpu().squeeze(), cmap=\"gray\", interpolation=\"nearest\")\n",
        "    title = f\"P: {int(preds[i])}\"\n",
        "    if preds[i] == test_labels[i]:\n",
        "        title += \" ✓\"\n",
        "    else:\n",
        "        title += f\" ✗ (T: {int(test_labels[i])})\"\n",
        "    ax.set_title(title, fontsize=8)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Wh8ntvidCDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}