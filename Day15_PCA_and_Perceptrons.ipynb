{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPLPybD68NjUopV4p7HFCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day15_PCA_and_Perceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day14\n",
        "##Principal Component Analysis & Perceptrons\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n"
      ],
      "metadata": {
        "id": "8TmDLmOfqmAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "00UWYEjFTVpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis Code\n",
        "\n",
        "Documentation: [`sklearn.decomposition.PCA()`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n"
      ],
      "metadata": {
        "id": "Nq4un7A2pn5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "U5lWilLspzJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/CS167/datasets/irisData.csv\")\n",
        "predictors = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
        "target = \"species\"\n"
      ],
      "metadata": {
        "id": "DPoZvwb0p0tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, let's establish a baseline -- How well does KNN do with all of the predictors?"
      ],
      "metadata": {
        "id": "4dlEDJ6ewI8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "  train_test_split(iris_df[predictors], iris_df[target], test_size = 0.2, random_state=29) # yes, I picked state 29 on purpose...\n",
        "clf = KNeighborsClassifier()        # create classifier object\n",
        "clf.fit(train_data,train_sln)       # fit the training data\n",
        "predictions = clf.predict(test_data) # use the model to make predictions\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions)) # what is the accuracy?"
      ],
      "metadata": {
        "id": "D6zhVqmsqSnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, let's use PCA to reduce the dimensions to just 2 principal components"
      ],
      "metadata": {
        "id": "PhMsDVYywV1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#whiten = True is important for uncorrelated\n",
        "#attributes, and is False by default\n",
        "extractor = PCA(n_components=2, whiten=True)\n",
        "#When fitting with PCA, you do not use the target column - this is an unsupervised learning algorithm\n",
        "extractor.fit(train_data)\n",
        "\n",
        "print('this is the variance/importance of each component')\n",
        "print(extractor.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "fHUgJYYpguBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Here's what the data looks like before being transformed:\")\n",
        "train_data[0:4]"
      ],
      "metadata": {
        "id": "Q8Sdfekkg3OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transformed = extractor.transform(train_data)\n",
        "\n",
        "print(\"Here's what the training predictors look like after the transformation.\")\n",
        "train_transformed[0:4]"
      ],
      "metadata": {
        "id": "86Z2grDcg6FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we transform the predictor columns in the test set as well.\n",
        "#Notice that we're using the extractor that we trained using the training set.\n",
        "#Do not re-fit it to the test data.\n",
        "test_transformed = extractor.transform(test_data)\n",
        "\n",
        "#Now we can use our transformed data with a classifier just like always:\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(train_transformed,train_sln)\n",
        "predictions = clf.predict(test_transformed)\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions))"
      ],
      "metadata": {
        "id": "8WNt2i8LhAFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the training data in terms of the new principal components\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#visualizing the new axes\n",
        "#PCA gives it back as numpy array\n",
        "tdf = pd.DataFrame(train_transformed)\n",
        "#next line: probably not the best way\n",
        "tdf['species'] = pd.Series(list(train_sln))\n",
        "\n",
        "\n",
        "setosa_series = tdf[ tdf['species'] == 'Iris-setosa' ]\n",
        "virginica_series = tdf[ tdf['species'] == 'Iris-virginica' ]\n",
        "versicolor_series = tdf[ tdf['species'] == 'Iris-versicolor']\n",
        "\n",
        "plt.plot(setosa_series[0],setosa_series[1],'ro',label='setosa')\n",
        "plt.plot(virginica_series[0],virginica_series[1],'bs',label='virginica')\n",
        "plt.plot(versicolor_series[0],versicolor_series[1],'g^',label='versicolor')\n",
        "plt.legend(loc='upper center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LqlkhlAbhIIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA Code (all together)"
      ],
      "metadata": {
        "id": "RYoZLV_chacd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# build your PCA extractor, decide how many output components (columns) you'd like\n",
        "extractor = PCA(n_components=2, whiten=True)\n",
        "\n",
        "#When fitting with PCA, you do not use the target column - this is an unsupervised learning algorithm\n",
        "extractor.fit(train_data)\n",
        "\n",
        "# transform your train_data and _test_data\n",
        "train_transformed = extractor.transform(train_data)\n",
        "test_transformed = extractor.transform(test_data)\n",
        "\n",
        "#build and test your model\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(train_transformed,train_sln)\n",
        "predictions = clf.predict(test_transformed)\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions))"
      ],
      "metadata": {
        "id": "JfgZIF54hdab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’¬ Group Exercise:\n",
        "\n",
        "- Download the the `boston_housing.csv` dataset from blackboard.\n",
        "- See the code below to read in the data file and create the training and testing dataframes\n",
        "\n",
        "In Blackboard, enter your answers for the the following R<sup>2</sup> metrics rounded to the nearest .01.\n",
        "\n",
        "1. Establish a baseline. Without doing PCA, what is the R<sup>2</sup> metric for a **random forest** regressor, with `random_state = 4`?\n",
        "2. Use PCA (as a pre-processing step) and a **random forest** regressor (with `random_state = 4`), what is the R<sup>2</sup> metric with using\n",
        "   - 2 principal components?\n",
        "   - 4 principal components?\n",
        "   - 6 principal components?"
      ],
      "metadata": {
        "id": "fBlKgDMJytad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# load the data\n",
        "housing_df = pd.read_csv(\"/content/drive/MyDrive/CS167/datasets/boston_housing.csv\")\n",
        "predictors = housing_df.columns.drop(\"MEDV\")\n",
        "target = \"MEDV\"\n",
        "\n",
        "#split the data\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "       train_test_split(housing_df[predictors], housing_df[target], test_size = 0.2, random_state=41)"
      ],
      "metadata": {
        "id": "A6CNvaKlhyLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}