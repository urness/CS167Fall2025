{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13856771,"sourceType":"datasetVersion","datasetId":8827296}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project #2\n## (your name here)\n\n#### CS167: Machine Learning, Fall 2025","metadata":{}},{"cell_type":"markdown","source":"## __Put the Model in GPU mode__\n\nWe want to accelerate the training process using graphical processing unit (GPU). You need to enable it (click Settings --> Accelerator--> GPU T4 x2)","metadata":{}},{"cell_type":"code","source":"import torch\n# check GPU (Kaggle will show \"cuda\" if GPU enabled in the notebook settings)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\n\n# Set seeds for reproducibility\nseed = 42  # you can choose any integer\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# If using CUDA:\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if using multi-GPU","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 1: imports and device (Kaggle version)\n# ============================================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets, models\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport time\nimport os\n# ===========================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: dataset paths (Kaggle version)\n# ============================================\n\nbase_dir   = \"/kaggle/input/bfgmss-v1\"      # <-- change this name to match your Kaggle dataset\ntrain_dir  = os.path.join(base_dir, \"bfgmss_v1/train\")\ntest_dir   = os.path.join(base_dir, \"bfgmss_v1/test\")\n\n# For AlexNet: normalize with ImageNet mean/std and resize to 227x227\ntransform = transforms.Compose([\n    transforms.Resize((227, 227)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        (0.485, 0.456, 0.406),\n        (0.229, 0.224, 0.225)\n    )\n])\n\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntest_dataset  = datasets.ImageFolder(test_dir,  transform=transform)\n\ndataset_labels = train_dataset.classes\nnumber_of_classes = len(dataset_labels)\nprint(\"Classes:\", dataset_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 3: define Neural Network model here\n# ============================================\n\nimport torch\nimport torch.nn as nn\n\nclass SimpleMLP(nn.Module):\n    \"\"\"\n    MLP for 150x150 RGB images.\n    - Optional AdaptiveAvgPool2d to reduce dimensionality before flattening.\n    - 2 hidden layers + final classifier.\n    \"\"\"\n    def __init__(\n        self,\n        num_classes: int = 6\n    ):\n        super().__init__()\n\n        # Input [B,3,150,150] -> [B,3,pooled_hw,pooled_hw]\n        self.shrink = nn.AdaptiveAvgPool2d((32, 32))\n        in_features = 3 * 32 * 32\n      \n        self.flatten = nn.Flatten()\n\n        self.network_layers = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.shrink(x)     # optional spatial averaging\n        x = self.flatten(x)    # [B, N]\n        return self.network_layers(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 4: training / testing loops \n# ============================================\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    model.train()\n    size = len(dataloader.dataset)\n    running_loss = 0.0\n    correct = 0\n\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # forward + loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # backward + update\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        running_loss += loss.item()\n        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = correct / size\n    return avg_loss, accuracy\n\ndef test_loop(dataloader, model, loss_fn):\n    model.eval()\n    size = len(dataloader.dataset)\n    running_loss = 0.0\n    correct = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n\n            running_loss += loss.item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n            all_preds.append(pred.argmax(1).cpu())\n            all_labels.append(y.cpu())\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = correct / size\n\n    all_preds = torch.cat(all_preds)\n    all_labels = torch.cat(all_labels)\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n\n    return avg_loss, accuracy, conf_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 5: your fine-tuning block (Kaggle-ready)\n# ============================================\nmlp_model = SimpleMLP(number_of_classes)\nmlp_model.to(device)\nprint(mlp_model)\n\nlearning_rate   = 1e-4\nbatch_size_val  = 32\nepochs          = 10\nloss_fn         = nn.CrossEntropyLoss()\noptimizer       = optim.Adam(mlp_model.parameters(), lr=learning_rate)\nsoftmax         = nn.Softmax(dim=1)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size_val,\n    shuffle=True,\n    num_workers=2,        # Kaggle: use workers to speed up loading\n    pin_memory=True if device == \"cuda\" else False\n)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=batch_size_val,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True if device == \"cuda\" else False\n)\n\ntrain_losses = []\ntest_losses  = []\ntrain_accuracies = []\ntest_accuracies  = []\n\nstart_time = time.time()\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    avg_train_loss, train_accuracy = train_loop(train_dataloader, mlp_model, loss_fn, optimizer)\n    avg_test_loss, test_accuracy, conf_matrix_test = test_loop(test_dataloader, mlp_model, loss_fn)\n\n    train_losses.append(avg_train_loss)\n    test_losses.append(avg_test_loss)\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)\n\n    print(f\"Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}\")\n    print(f\"Test  loss: {avg_test_loss:.4f}, Test  acc: {test_accuracy:.4f}\")\n\nprint(\"MLP model has been fine-tuned!\")\ntotal_time_sec = time.time() - start_time\nprint(\"Total fine-tuning time: %.3f sec\" % total_time_sec)\nprint(\"Total fine-tuning time: %.3f hrs\" % (total_time_sec / 3600.0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualizing the accuracy curves\nplt.plot(range(1,epochs+1), train_accuracies)\nplt.plot(range(1,epochs+1), test_accuracies)\nplt.title('Model accuracies after each epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show confusion matrix for final epoch\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_test, display_labels=dataset_labels)\ndisp.plot(xticks_rotation=45,cmap=\"Blues\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\nAdditional starter neural networks to consider...\n---","metadata":{}},{"cell_type":"code","source":"# ============================================\n# Step 3: define Neural Network model here\n# ============================================\nimport torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    \"\"\"\n    SimpleCNN for RGB images.\n    - 2 Conv2d layers (with ReLU + MaxPool)\n    - 2-layer MLP head\n    \"\"\"\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n\n        # Feature extractor: keep it small & fast for Kaggle\n        self.conv_layers = nn.Sequential(\n            # Input: [B, 3, 150, 150]\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # -> [B, 32, 150, 150]\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),                              # -> [B, 32, 75, 75]\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), # -> [B, 64, 75, 75]\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),                              # -> [B, 64, 37, 37]\n        )\n\n        # Make the spatial size fixed before the MLP (no fragile hard-coding)\n        self.spatial_pool = nn.AdaptiveAvgPool2d((7, 7))  # -> [B, 64, 7, 7]\n\n        self.flatten = nn.Flatten()                       # -> [B, 64*7*7]\n        self.linear_layers = nn.Sequential(\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes)                   # e.g., 6 classes\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.spatial_pool(x)\n        x = self.flatten(x)\n        x = self.linear_layers(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Step 3: define Neural Network model here\n# ============================================\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(AlexNet, self).__init__()\n        net = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1 if pretrained else None)\n\n        # retain convolutional and pooling layers\n        self.features = net.features\n        self.avgpool  = net.avgpool\n\n        # replace classifier with new head for our num_classes\n        self.classifier = nn.Sequential(\n            nn.Linear(256 * 6 * 6, 128),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)          # feature extraction from AlexNet\n        x = self.avgpool(x)           # spatial pooling from AlexNet\n        x = torch.flatten(x, 1)       # flatten to (batch_size, feature_dim)\n        x = self.classifier(x)        # MLP for final classification\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}