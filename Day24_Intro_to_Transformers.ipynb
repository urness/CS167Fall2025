{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpE9V5DUOeoO7n21WUxobQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day24_Intro_to_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day24\n",
        "## Intro to Transformers\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n",
        "\n"
      ],
      "metadata": {
        "id": "TCt5SH6SzJuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Credit__:\n",
        "\n",
        "Much of the code and lecture materials used from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "Free online course: [How Transformers Work](https://learn.deeplearning.ai/courses/how-transformer-llms-work)\n"
      ],
      "metadata": {
        "id": "B7e3F5trzPV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "\n",
        "\n",
        "It's not necessary to have GPU for this notebook. However, it won't hurt.\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime (or click on the down arrow near RAM & DISK in upper right)-->Change runtime type-->GPU or TPU_\n",
        "\n",
        "Professor Urness tested this code with the GPU option: T4"
      ],
      "metadata": {
        "id": "jSIsZsKfzk4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some necessary import statements:"
      ],
      "metadata": {
        "id": "yLyFOwWQ0EBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers>=4.46.1\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "fmxGaJ_az_Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tokenizing Text"
      ],
      "metadata": {
        "id": "t3qEiONe0WKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will tokenize the sentence \"Hello World!\" using the tokenizer of the [`bert-base-cased` model](https://huggingface.co/google-bert/bert-base-cased).\n",
        "\n",
        "Let's import the `Autotokenizer` class, define the sentence to tokenize, and instantiate the tokenizer."
      ],
      "metadata": {
        "id": "Uia0cFU-0Zvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>FYI: </b> The transformers library has a set of Auto classes, like AutoConfig, AutoModel, and AutoTokenizer. The Auto classes are designed to automatically do the job for you.</p>"
      ],
      "metadata": {
        "id": "-sm_BbgR0ezP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# define the sentence to tokenize\n",
        "sentence = \"Hello world!\""
      ],
      "metadata": {
        "id": "J1-Le8hg0jh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "wjlzoONd0zti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll now apply the tokenizer to the sentence. The tokeziner splits the sentence into tokens and returns the IDs of each token."
      ],
      "metadata": {
        "id": "a6tlaDV-0tvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the tokenizer to the sentence and extract the token ids\n",
        "token_ids = tokenizer(sentence).input_ids"
      ],
      "metadata": {
        "id": "LF41gmkk0wdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the token ids\n",
        "print(token_ids)"
      ],
      "metadata": {
        "id": "OJENlmWW05NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To map each token ID to its corresponding token, you can use the `decode` method of the tokenizer.\n",
        "for id in token_ids:\n",
        "    print(tokenizer.decode(id))"
      ],
      "metadata": {
        "id": "OCieiZxs08mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__What is `[SEP]`??__\n",
        "\n",
        "In the bert-base-cased tokenizer, the special token [SEP] is a separator token used to mark the end of a sentence or separate two segments in a pair of inputs."
      ],
      "metadata": {
        "id": "ZcphaufQ1LU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise #1\n",
        "Using the bert-base-cased tokenizer, what is the token number assigned to the word \"Thanksgiving\"?"
      ],
      "metadata": {
        "id": "uaDpIO8M1MiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise #2\n",
        "\n",
        "Using the bert-base-cased tokenizer, how many tokens does it require to represent the word 'discombobulated' (not including the CLS or SEP tokens)?\n"
      ],
      "metadata": {
        "id": "wihgD0yi17sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Tokenization\n",
        "\n",
        "In this section, you'll use the provided function `show_tokens`. The function takes in a text and the model name, and prints the vocabulary length of the tokenizer and a colored list of the tokens."
      ],
      "metadata": {
        "id": "EZUXj9a32rST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list of colors in RGB for representing the tokens\n",
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence: str, tokenizer_name: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ],
      "metadata": {
        "id": "JHAa8ap_2550"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the text that you'll use to explore the different tokenization strategies of each model. Notice how complicated it is, including strange characters and icons. This string will illustrate how different tokenizers handle these cases."
      ],
      "metadata": {
        "id": "FgdIzSBm2-BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµ é¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FygU-vvh2_-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note how the bert-based-cased tokenization breaks down words to tokenize them\n",
        "show_tokens(text, \"bert-base-cased\")"
      ],
      "metadata": {
        "id": "r7iJ5QDj3RwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise #3\n",
        "Go to https://huggingface.co/\n",
        "\n",
        "- Search for models\n",
        "- Explore running the `show_tokens(text, \"model_name_goes_here\")` on various models\n",
        "\n",
        "What is the vocbulary length of the `Qwen/Qwen2-VL-7B-Instruct` tokenizer?"
      ],
      "metadata": {
        "id": "YkV31y0N5iue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"openai/gpt-oss-20b\")"
      ],
      "metadata": {
        "id": "xuf_Mk5229TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Word Embeddings__\n",
        "\n"
      ],
      "metadata": {
        "id": "NCXiZ8PjAB-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with different pairs of words.  "
      ],
      "metadata": {
        "id": "TwDd4Z5_Kd7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Load a small pretrained embedding model from Gensim\n",
        "# ----------------------------------------------------------\n",
        "w2v = api.load(\"glove-wiki-gigaword-50\")   # or 100, 200, 300"
      ],
      "metadata": {
        "id": "anyYkJ7GOVfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Words\n",
        "# ----------------------------------------------------------\n",
        "word1 = \"dog\"\n",
        "word2 = \"puppy\"\n",
        "\n",
        "vec1 = torch.tensor(w2v[word1])\n",
        "vec2 = torch.tensor(w2v[word2])\n",
        "\n",
        "print(\"Vec1 shape:\", vec1.shape)\n",
        "print(\"Vec2 shape:\", vec2.shape)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Cosine similarity\n",
        "# ----------------------------------------------------------\n",
        "cos_sim = F.cosine_similarity(vec1, vec2, dim=0).item()\n",
        "print(f\"\\nCosine similarity: {cos_sim:.4f}\")"
      ],
      "metadata": {
        "id": "tWrmP8yQM7ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Word Embeddings with Context__"
      ],
      "metadata": {
        "id": "02b9OqdpO4HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Load a pretrained tokenizer + model (BERT)\n",
        "# ----------------------------------------------------------\n",
        "# The tokenizer maps text â†’ tokens â†’ token IDs\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# The model produces contextual embeddings for each token\n",
        "# (each token gets a 768-dimensional vector based on the surrounding words)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Two sentences where \"bank\" means different things\n",
        "# ----------------------------------------------------------\n",
        "s1 = \"He sat by the bank of the river.\"\n",
        "s2 = \"I went to the bank to deposit money.\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Tokenize and get embeddings for the target word\n",
        "# ----------------------------------------------------------\n",
        "def get_word_embedding(sentence, word):\n",
        "    # Tokenize the sentence, adding CLS, SEP, and returning PyTorch tensors\n",
        "    tokens = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "    # Run the tokens through BERT â€” outputs last_hidden_state:\n",
        "    # shape = [1, seq_len, 768]\n",
        "    outputs = model(**tokens)\n",
        "\n",
        "    # Remove the batch dimension â†’ [seq_len, 768]\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Convert token IDs back to readable token strings\n",
        "    tokens_decoded = tokenizer.convert_ids_to_tokens(tokens['input_ids'].squeeze(0))\n",
        "\n",
        "    # Find the index where the target word appears in the token list\n",
        "    # (lowercasing used since this is an uncased model)\n",
        "    idx = tokens_decoded.index(word)\n",
        "\n",
        "    # Return the 768-dimensional embedding vector and the token list\n",
        "    return embeddings[idx], tokens_decoded\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Get the contextual embedding for \"bank\" in both sentences\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "emb1, tokens1 = get_word_embedding(s1, \"bank\")\n",
        "emb2, tokens2 = get_word_embedding(s2, \"bank\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. Compare the two contextual embeddings\n",
        "# ----------------------------------------------------------\n",
        "# Cosine similarity close to 1 â†’ vectors are similar (same meaning)\n",
        "# Cosine similarity close to 0 â†’ vectors are different\n",
        "similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "print(\"Tokens 1:\", tokens1)\n",
        "print(\"Tokens 2:\", tokens2)\n",
        "print(f\"\\nCosine similarity between 'bank' embeddings: {similarity:.4f}\")\n"
      ],
      "metadata": {
        "id": "tEGTrYfTLTnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Context Window__"
      ],
      "metadata": {
        "id": "pn0knU61SIgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import values\n",
        "!pip install transformers torch --quiet\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM\n",
        ")\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "cItWOdDZSaI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilgpt2\"  # small GPT-style decoder-only model\n",
        "\n",
        "dec_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dec_model     = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# For GPT2-like models we often set pad_token = eos_token\n",
        "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
        "dec_model.config.pad_token_id = dec_tokenizer.eos_token_id\n",
        "\n",
        "prompt_short = \"My favorite color is blue. Question: What is my favorite color? Answer:\"\n",
        "prompt_long  = \"My favorite color is blue. \" + \"blah \" * 80 + \"Question: What is my favorite color? Answer:\"\n",
        "\n",
        "def generate_and_print(prompt):\n",
        "    inputs = dec_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = dec_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    print(\"PROMPT:\\n\", prompt)\n",
        "    print(\"COMPLETION:\\n\", dec_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "generate_and_print(prompt_short)\n",
        "generate_and_print(prompt_long)\n"
      ],
      "metadata": {
        "id": "zp2PlEbCSQXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}