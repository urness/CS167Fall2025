{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+ydRiHXX5Iz9Ptp7xn3K6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall2025/blob/main/Day14_Random_Forests_and_Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS167: Day14\n",
        "##Random Forests & Dimensionality Reduction Techniques\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2025\n"
      ],
      "metadata": {
        "id": "8TmDLmOfqmAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "00UWYEjFTVpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0. import libraries\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "data= pd.read_csv(\"/content/drive/MyDrive/CS167/datasets/breast-cancer-data.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Ep7V80K5JoQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data\n",
        "target = \"diagnosis\"\n",
        "predictors = data.columns.drop(target) #gets all of the columns except the target\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(data[predictors], data[target], test_size = 0.2, random_state=41)"
      ],
      "metadata": {
        "id": "nas9BvJ1TZBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Code"
      ],
      "metadata": {
        "id": "AuBfXb2jFqpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a Random Forest Classifier\n",
        "forest = RandomForestClassifier(random_state = 0, max_features=\"log2\")\n",
        "forest.fit(train_data,train_sln)\n",
        "predictions = forest.predict(test_data)\n",
        "print(\"accuracy score: \", metrics.accuracy_score(test_sln,predictions))\n",
        "\n",
        "vals = data[target].unique() ## possible classification values (M = malignant; B = benign)\n",
        "conf_mat = metrics.confusion_matrix(test_sln, predictions, labels=vals)\n",
        "print(pd.DataFrame(conf_mat, index = \"True \" + vals, columns = \"Predicted \" + vals))"
      ],
      "metadata": {
        "id": "fG_QyZRQVcrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importances"
      ],
      "metadata": {
        "id": "wdHG3wFPcFnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It looks like our random forest model achieved pretty good accuracy.\n",
        "# Now lets check how important each of the features was in the ensemble of models we built.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#creates a list of numbers the right size to use as the index\n",
        "#and sorts the list so that the most important feature are first\n",
        "index = range(len(predictors))\n",
        "importances = forest.feature_importances_\n",
        "sorted_indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(8,10)) #making the table a bit bigger so the text is readable\n",
        "plt.title('Breast Cancer Feature Importances')\n",
        "plt.barh(range(len(sorted_indices)),importances[sorted_indices],height=0.8) #horizontal bar chart\n",
        "plt.ylabel('Feature')\n",
        "plt.yticks(index,predictors) #put the feature names at the y tick marks\n",
        "plt.xlabel(\"Random Forest Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aNQ4fpIWcLol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning the Forest\n",
        "\n",
        "*   How can we tell how many trees to use?\n",
        "*   What about how many features to include in our trees?\n",
        "\n",
        "We can tune our random forest to find the best values of model\n",
        "parameters:\n",
        "\n"
      ],
      "metadata": {
        "id": "L1_-VF40c4eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function just loops through a series of n_estimator (number of trees) values, builds a different model\n",
        "#for each, and then plots their respective accuracies. By making it a function, it's easier\n",
        "#to try out different ranges of numbers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def tune_number_of_trees(n_estimator_values):\n",
        "    rf_accuracies = []\n",
        "\n",
        "    # loop through all of the possible number of trees\n",
        "    for n in n_estimator_values:\n",
        "\n",
        "        curr_rf = RandomForestClassifier(n_estimators=n, random_state=0)     # create classifier object\n",
        "        curr_rf.fit(train_data,train_sln)                                    # fit model to training data\n",
        "        curr_predictions = curr_rf.predict(test_data)                        # use model to make predictions\n",
        "        curr_accuracy = metrics.accuracy_score(test_sln,curr_predictions)    # compare predictions to test solutions to determine accuracy\n",
        "        rf_accuracies.append(curr_accuracy)                                  # add accuracy to list\n",
        "\n",
        "    # now let's plot the accuracies\n",
        "    plt.suptitle('Random Forest accuracy vs. number of trees',fontsize=18)\n",
        "    plt.xlabel('# trees')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.plot(n_estimator_values,rf_accuracies,'ro-')\n",
        "    plt.axis([0,n_estimator_values[-1]+1,.9,1.01])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "tune_number_of_trees(range(1,31))"
      ],
      "metadata": {
        "id": "NNDW9l7SdGAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like whether we are using small numbers of trees or large ones, the accuracy stays about the same. It appears at least sometimes that Random Forest doesn't take a lot of tuning of the number of trees."
      ],
      "metadata": {
        "id": "mYC81K4QdQ2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning Number of Features"
      ],
      "metadata": {
        "id": "EPqQHKXWd3jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function just loops through a series of max_features (assuming the number of tree is set at 10), builds a different model\n",
        "#for each, and then plots their respective accuracies. By making it a function, it's easier\n",
        "#to try out different ranges of numbers\n",
        "def tune_max_features(max_features_values):\n",
        "    rf_accuracies = []\n",
        "\n",
        "    # loop through the number of max features\n",
        "    for m in max_features_values:\n",
        "\n",
        "        curr_rf = RandomForestClassifier(n_estimators=10,max_features=m, random_state=0) # create classifier object\n",
        "        curr_rf.fit(train_data,train_sln)                                                # fit model to training data\n",
        "        curr_predictions = curr_rf.predict(test_data)                                    # use model to make predictions\n",
        "        curr_accuracy = metrics.accuracy_score(test_sln,curr_predictions)                # compare predictions to test solutions to determine accuracy\n",
        "        rf_accuracies.append(curr_accuracy)                                              # add accuracy to list\n",
        "\n",
        "    # now let's plot the accuracies\n",
        "    plt.suptitle('Random Forest accuracy vs. max features',fontsize=18)\n",
        "    plt.xlabel('max features')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.plot(max_features_values,rf_accuracies,'ro-')\n",
        "    plt.axis([0,max_features_values[-1]+1,.9,1.01])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "tune_max_features(range(1,11))"
      ],
      "metadata": {
        "id": "zK0I25tUd2_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection Code\n",
        "\n",
        "Documentation: [`sklearn.feature_selection.SelectKBest()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)"
      ],
      "metadata": {
        "id": "Nq4un7A2pn5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "U5lWilLspzJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/CS167/datasets/irisData.csv\")\n",
        "predictors = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
        "target = \"species\"\n"
      ],
      "metadata": {
        "id": "DPoZvwb0p0tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "    train_test_split(iris_df[predictors], iris_df[target], test_size = 0.2, random_state=41)"
      ],
      "metadata": {
        "id": "D6zhVqmsqSnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, let's establish a baseline -- How well does KNN do with all of the predictors?"
      ],
      "metadata": {
        "id": "4dlEDJ6ewI8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = KNeighborsClassifier()        # create classifier object\n",
        "clf.fit(train_data,train_sln)       # fit the training data\n",
        "predictions = clf.predict(test_data) # use the model to make predictions\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions)) # what is the accuracy?"
      ],
      "metadata": {
        "id": "U1VAk0RDwSas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, let's just select the best 2 predictors"
      ],
      "metadata": {
        "id": "PhMsDVYywV1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit your selector just like you do when training with a classifier/regressor\n",
        "# only do this after splitting into train and test sets - don't let the test\n",
        "# set spoil your predictions\n",
        "selector = SelectKBest(k=2)\n",
        "selector.fit(train_data,train_sln)\n",
        "\n",
        "# bigger number means the feature is more important\n",
        "print('Here are the scores of each feature:')\n",
        "print(selector.scores_)\n",
        "print(predictors)"
      ],
      "metadata": {
        "id": "Ltyh5AADwgxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforming the predictor columns of the training set\n",
        "train_transformed = selector.transform(train_data)\n",
        "\n",
        "print(\"Here's what the training predictors look like after the transformation. \\\n",
        "Notice that it's just the last two columns from the original data.\")\n",
        "train_transformed[0:6]"
      ],
      "metadata": {
        "id": "jjhUV890w2eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#take a look at the training data\n",
        "train_data[0:6]"
      ],
      "metadata": {
        "id": "AlooXVmjxURb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we transform the predictor columns in the test set as well.\n",
        "#Notice that we're using the selector that we trained using the training set.\n",
        "#Do not re-fit it to the test data.\n",
        "test_transformed = selector.transform(test_data)\n",
        "\n",
        "#Now we can use our transformed data with a classifier just like always:\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(train_transformed,train_sln)\n",
        "predictions = clf.predict(test_transformed)\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions))"
      ],
      "metadata": {
        "id": "tCzs3VnrxbHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💬 Group Exercise:\n",
        "\n",
        "Let's give it a shot:\n",
        "- below, I went ahead and loaded (and cleaned) the penguin dataset 🐧\n",
        "- Using `species` as the target variable, Use `SelectKBest` to determine the best 3 attributes\n",
        "- Build a default Random Forest using only the 3 best attributes. How does the performance compare to a default random forest that uses all of the predictor variables?\n",
        "- Keep running the code, incrementing the number of predictors. What is the minimum number of predictors needed to get 100% accuracy?\n"
      ],
      "metadata": {
        "id": "fBlKgDMJytad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## the following code will load and clean the penguin dataset and will result in 8 predictors\n",
        "penguin_df = pd.read_csv(\"/content/drive/MyDrive/CS167/datasets/penguins.csv\")\n",
        "penguin_df.head()\n",
        "penguin_df.dropna(inplace=True) # drop null values\n",
        "penguin_df[\"gender\"] = penguin_df[\"gender\"].map({\"MALE\": 0, \"FEMALE\": 1})\n",
        "penguin_df = pd.get_dummies(penguin_df, columns=[\"island\"]) # one-hot encode the data\n",
        "penguin_df.head()"
      ],
      "metadata": {
        "id": "ZWVW7XUxy2BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"species\"\n",
        "predictors = penguin_df.columns.drop(target)\n",
        "\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "    train_test_split(penguin_df[predictors], penguin_df[target], test_size = 0.2, random_state=41)"
      ],
      "metadata": {
        "id": "FZCO5Git06N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Establish the base case, using all of the predictors in a Random Forest"
      ],
      "metadata": {
        "id": "eMzDtrQh97wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(train_data,train_sln)\n",
        "predictions = rf.predict(test_data)\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions))"
      ],
      "metadata": {
        "id": "8b2nDUfa1BBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Your code here;\n",
        "## Use SelectKBest and start with 3 predictors;\n",
        "## Keep running your code, incremented the number of predictors. What is the minimum number of predictors needed to get 100% accuracy?"
      ],
      "metadata": {
        "id": "89KRlwD3-ENH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}